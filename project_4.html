
<!DOCTYPE html>
<html>
	<head>
		<!-- Google tag (gtag.js) -->
<!-- 		<script async src="https://www.googletagmanager.com/gtag/js?id=G-P9JHZPCK6Q"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());
		
		  gtag('config', 'G-P9JHZPCK6Q');
		</script> -->
		<meta charset="UTF-8"/>
		<title>My projects</title>
		<link rel="icon" type="image/x-icon" href="img/favicon.ico">
		<meta name = "author" content = "Han Zhang"/>
		<meta name = "keywords" content = "Zhang, Han, Data analyst, Teacher, Computer, Science, Personal"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=yes"/>
		<link rel = "stylesheet" href="style.css"/>
		<script src = "js/app.js"></script>
		<script type="text/javascript" src="https://webpivottable.com/releases/latest/dist/wpt.js" ></script>
	</head>

	<body>
		<div class = "container">
			<div class = "header">
				<h1>My Projects</h1>
				<h1>Mon projets</h1>
			</div>
			<br />
			<div align = "center">
				<a href = "myJob.html"> About me </a> |
				<a href = "aboutMe.html"> More about me </a> |
				<a href = "contactMe.html"> Contact </a> |
				<a href = "myProject.html"> Projects</a>
			</div>

		
		<br>
		<li><b>Job market analysis in selected Canadian cities - on going</b> </li>
		     <br>
		     <br>
			This project aims at investigating the job supplies and categories in target cities in Canada (Sherbrooke, Montreal, Quebec City, Toronto, Vancouver, Calgary, Halifax) in a certain period. The raw data is pulled from Adzuna. It's an ongoing project, the raw data will be updated every Friday (started from 2024-11-22), 
			and it will be saved in the SQL Server on my local PC. The data will be transfered to Power BI to be analyzed and visualized. It's also a "trial and error" project, which means the algorithm will probably be adjusted and updated based on the results accordingly. Jobs will be categorized into several groups, for example, Agriculture, Customer Services, Financial Services, IT, Education, etc. (there are 30 categories in Adzuna). 
			The results will show the percentage of each category in a city, say, in the past week, among the 2000 random samples (job posts) from Montreal, 40% of the job falls into customer service; 30% falls into financial services; 20% falls into IT, etc. 
			This project is different than the <a href="project_1.html">"Top 5 companies in Canada for data analyst"</a>, where the analysis is done by the server of Adzuna, in this project all the analysis will be done by myself. Adzuna API will just supply the raw data. The following diagram shows the general procedures of this project.
		    <br><br>
			<img src="/img/job_maket_1.png" alt="work flow" width="800" height="210" class="responsive-logo"> <br>

			
			<b>1. Extract raw data from Adzuna API by Python</b>
			<br><br>
			The raw data will be pulled from a third-party platform – Adzuna, which is same as in the first project. <br>
			A key thing to clarify in this step is the sampling technique, it's the foundation of this project. Enough sample size will give me a good understanding of the job market with as little bias as possible. However, how much is "enough"? I don't know.
			So I'll try some small samples as a starting point at first, if the outcome is not ideal, I will adjust the sample size then repeat until I get a good response. Given the population of my target cities differ a lot, choosing a same sample size (for example 1000) will probably exhaust all job posts in that 
			period in small cities like Sherbrooke, but will be a bit small and not representitive enough for big cities like Toronto. Therefore, I will use proportional sampling technique. One thing to consider is that jobs posted in Quebec province are in French (some are in English), when parsing the website, keep an eye on some relevant keywords to see if they need to be adjusted. Here comes the detail: 
			<ol>
				<li>Set up a maximum cap for the largest city – Toronto, say 2000 job posts. (Why 2000? - as per Adzuna API, started from 2024-11-22, the total job posts of Toronto (within 30 km radius) in the past 7 days are 5197. 
				    Let's suppose Adzuna can cover all the job posts in that week in Toronto, then 5197 is the “population” in statistics from 7 days ago till that day, so 2000 sample data represents 38% of the entire jobs, which is a good enough.) In addition, this sample size would still reflect job diversity but with fewer API calls considering the tradeoff between the dada sampling and traffic on the server of Adzuna. </li><br>
				<li>Calculate the scaling factor. Say the population of Toronto city is approximately 2,794,356 according to <a href = "https://www12.statcan.gc.ca/census-recensement/2021/dp-pd/prof/details/page.cfm?Lang=E&GENDERlist=1,2,3&STATISTIClist=1,4&HEADERlist=0&DGUIDlist=2021A00053520005&SearchText=toronto" target="_blank">Statistics Canada of 2021 Census of Population</a>
				<br><br>
					<img src="/img/jm_f1.png" alt="work flow" width="400" height="110" class="responsive-logo"> 
				</li><br>
				<li>Apply this factor (f = 0.0007) to other cities to get the sample size:<br>
				Montreal, QC – 1762949 × 0.0007 ≈ 1262,<br>
				Calgary, AB – 1306784 × 0.0007 ≈ 935,<br>
				Vancouver, BC – 662248 × 0.0007 ≈ 474,<br>
				Quebec City, QC – 549459 × 0.0007 ≈ 393,<br>
				Halifax, NS – 439819 × 0.0007 ≈ 315,<br>
				Sherbrooke, QC – 172950 0.0007 ≈ 124<br>
				<br>
				let me define sample size of each city using a dictionary comprehension (for loop in key-value pair of a dictionary) <br>
<pre>
city_population = {
    'Toronto,ON': 2794356,
    'Montreal,QC': 1762949,
    'Calgary,AB': 1306784,
    'Vancouver,BC': 662248,
    'Quebec City,QC': 549459,
    'Halifax,NS': 439819,
    'Sherbrooke,QC': 172950,
}
</pre>
					<code>samples_per_city = {city: min(int(pop * scaling_factor), toronto_cap) for city, pop in city_population.items()}</code><br><br>
				the results will be like:<br>
<pre>
{'Toronto,ON': 1999,
 'Montreal,QC': 1261,
 'Calgary,AB': 935,
 'Vancouver,BC': 473,
 'Quebec City,QC': 393,
 'Halifax,NS': 314,
 'Sherbrooke,QC': 123}
</pre>
				</li><br>
				<li>Confirm page number of each city (pagination). The sample size of each city has been confirmed in previous step. However, in Adzuna API, only the data in the first page will be pulled by default, and one page can have at most 50 records, which is apparently not enough.
				So we will need to configure the "page" and “results_per_page” parameters in Adzuna. Here “page” refers to the page number, it can be integers like 1 or 2, etc.. And “results_per_page” indicates the number of job posts in one page. 
				By default, it’s 20, and the maximum number is 50. So, for a city like Toronto with sample size 2000. If I have 50 “results_per_page”, then I’ll need to pull 40 pages. (Here I can consider using a for loop in Python).  Let me generalize it as:<br>
				<br>
				<img src="/img/jm_f2.png" alt="work flow" width="300" height="110" class="responsive-logo">  <br>
				If the “results_per_page” is 50, here comes the number of pages of each city (let’s round it up):<br>
				Number of pages (Toronto) = 2000 ÷ 50 = 40, <br>
				Number of pages (Montreal) = 1262 ÷ 50 ≈ 26, <br>
				Number of pages (Calgary) = 935 ÷ 50 ≈ 19,<br>
				Number of pages (Vancouver) = 474 ÷ 50 ≈ 10,<br>
				Number of pages (Quebec City) = 393 ÷ 50 ≈ 8,<br>
				Number of pages (Halifax) = 315 ÷ 50 ≈7, <br>
				Number of pages (Sherbrooke) = 124 ÷ 50 ≈3<br>
				<br>
				For this part, a for loop will solvoe this issue. So to start up, I will get a job list of the past 7 days from 2024-11-22 as a starting point, then follow up with weekly pulls (every Friday) for ongoing data collection, which will give me some insights into how the job market evolves over time (I will have time series analysis in the future). 
				This approach avoids the heavy overhead of daily pulls but is still detailed enough to capture changes in the job market. 	
				
				</li><br>
				<li>Set up the API parameters in Python and pull the data. With those key factors confirmed in the previous steps, it's time to set up the parameters. In Aduzna, some parameters work as authentication, such as APP_ID and APP_Key (the ID and Key in Adzuna), some work as filters to generate the filtered results as we need, these parameters can be appended to the base endpoint (URL). The original base endpoint looks like: 
				    <br>
				    https://api.adzuna.com/v1/api/jobs<br>
				    This base URL is same for all endpoints for accessing job listing. On top of that I have /ca, which indicates the country code (Canada in this case). Then I have /search/1 representing the search end point and page number (first page in this case). Then a question mark ? indicating the beginning of the query string, where the parameters are added. Till now, the endpoint is like: <br>
				    https://api.adzuna.com/v1/api/jobs/ca/search/1? <br>
				    As discussed above, 1 page is not enough to get the samples, so I’ll use a for loop to pull the data on the number of pages in each city as in 1.4. Other than that, we got following parameters to be appended after the question mark “?”:
<pre>
params = {
	    'app_id': APP_ID,
	    'app_key': APP_KEY,
	    'where': city (use a for loop to loop through ‘Toronto, Montreal, Vancouver, etc.)
	    'results_per_page': RESULTS_PER_PAGE, (here I set 50 – the max of Adzuna)
	    'max_days_old': MAX_DAYS_OLD, (since I’ll pull every week, so it’s 7)
	    'distance': DISTANCE, (it’s the distance to the city center in km, I set 30)
	    'sort_by': SORT_BY (I set sort by date – the most recent records will be put on top)
	}	
</pre>       

				   When I attach the parameters in the URL, I’ll make a request to GET the results:<br>
				   response = requests.get(api_url, params=params)<br>
				   Combining the parameters as above, it will look like:
<pre>
total_pages = math.ceil(sample_size / 50)
jobs = []

    for page in range (1, total_pages + 1):
        api_url = f'https://api.adzuna.com/v1/api/jobs/ca/search/{page}'
        params = {
            'app_id': APP_ID,
            'app_key': APP_KEY,
            'where': city,
            'results_per_page': RESULTS_PER_PAGE,
            'max_days_old': MAX_DAYS_OLD,
            'distance': DISTANCE,
            'sort_by': SORT_BY
        }
        
        response = requests.get(api_url, params=params)
</pre>
				   The returned results will be in the format of JSON, I’ll need to extract the value in the ‘key’- ‘value’ pair. The outputs I will get is as following:<br>
                                   Company name ("company",{"display_name":); Job title ("title"); Job Category ("Category","label":) (such as IT/ Accounting/Financial Services, etc.); Location (province, city)("location","area":); Education (required degree, such as bachelor in computer science); contract_time (full / part time) ("contract_time"); Contract type ("contract_type"); Salary ("salary_max", "salary_min"); Publish date ("created");Description("description":)
                                   then put them into SQL Server database. So here comes the next part …

				</li>
				<br>
				<b>2. Save data into SQL Server</b><br>
				After pulling the raw data, I'll save them into SQL Server database. The first thing I need to do is to CREATE DATABASE. Since I’ve got the key-value pair of each object in previous step, I’ll need to CREATE TABLE with the column names similar as the ‘key’ in the JSON file, for example, in the table, I need to have columns: company_name, job_tittle, job_category, province, city, etc, which are equivalent to the "company"."display_name", "title", "category"."label", 
				"location"."area"[1], "location"."area"[2]. etc. in the JSON file as I got in the previous step. 
				Here I'll use .get() function to access the values. <br>
				This is the code I wrote to parse JSON file and save them into database:
<pre>
connection_string = (
"Driver={ODBC Driver 17 for SQL Server};"
"Server=SERVER_NAME;"  # my server name, e.g., "localhost" or "192.168.1.100"
"Database=DATABASE_NAME;"
"Trusted_Connection=yes;"
)
    
    try:
        conn = pyodbc.connect(connection_string) # connect to SQL server
        cursor = conn.cursor()
        
        for job in jobs:
            cursor.execute('''
                INSERT INTO dbo.job_list (job_id, company_name, job_title, category, province, city, 
                contract_time, contract_type, salary_min, salary_max, publish_date, job_description, 
                total_job_count) 
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', 
                job.get('id'),
                job.get('company', {}).get('display_name'),
                job.get('title'),
                job.get('category', {}).get('label'),
                job.get('location', {}).get('area', [None, None])[1],
                job.get('location', {}).get('area', [None, None])[2],
                job.get('contract_time'),
                job.get('contract_type'),
                job.get('salary_min'),
                job.get('salary_max'),
                job.get('created'),
                job.get('description'),
                jobs_count)
        
        conn.commit()
        print(f"Successfully saved {len(jobs)} job(s) to the database.")
        
    except pyodbc.Error as e:
        print(f"An error occurred: {e}")
        
    finally:
        cursor.close()
        conn.close() 
</pre><br>
		        <b>3. ETL & data analysis in Power BI</b><br>
			Till now, the raw data is saved into database. Then I'll extract it and transform it to get the clean and ideal format for some features as I need. I'll do this part in Power Query, and then creat the models and dashboard in Power BI.<br>
			This is the first dashborad I created (I cannot publish my work with free license in PBI Desktop, so I just took a screenshot - it's pretty sloppy):<br><br>
			<img src="/img/job_maket_2.png" alt="job_market" width="700" height="300" class="responsive-logo"><br>
			From the results, I can at least know as of 2024-11-29, in Toronto, there are 10446 vacancies in total in the past 2 weeks, among which, the most demanding job category is IT - accounting for 20%; the second one is accounting & finance (13.2%). <br><br>
			Phew! Till now, I've got some results finally, but still, it's a rather general analysis. A lot of details are yet to be explored. Maybe there are some problems or errors that I haven't noticed yet - they could be exposed as I accumulate more data. <br>
			This post will be kept updating.
			
			<cite>References: <br>
				[1] Adzuna API (https://developer.adzuna.com/overview) <br>
				[2] RedHat (https://www.redhat.com/en/topics/api/what-is-a-rest-api)</cite>
							
		</div>
	</body>
</html>
